import os
from loguru import logger
import torch
import torch.distributed as dist
from nvidia.dali.pipeline import Pipeline
import nvidia.dali.types as types
from nvidia.dali.ops.readers import File
from nvidia.dali.ops.decoders import Image
from nvidia.dali.plugin.pytorch import DALIClassificationIterator
from nvidia.dali.plugin.base_iterator import LastBatchPolicy
from nvidia.dali.ops import RandomResizedCrop, CropMirrorNormalize, Resize
from nvidia.dali.ops.random import CoinFlip
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from ..conf import Config


class DaliImageNetTrainPipeline(Pipeline):
    def __init__(self,
                 batch_size: int,
                 data_dir: str,
                 interpolation: str,
                 crop: int,
                 seed: int,
                 dont_use_mmap: bool,
                 heterogeneous: bool):
        super().__init__(batch_size,
                         num_threads=4,
                         device_id=0,
                         seed=seed,
                         set_affinity=True)
        interpolation = {
            "bicubic": types.INTERP_CUBIC,
            "bilinear": types.INTERP_LINEAR,
            "triangular": types.INTERP_TRIANGULAR,
        }[interpolation]

        if dist.is_initialized() and (not heterogeneous):
            shard_id = dist.get_rank()
            num_shards = dist.get_world_size()
        else:
            shard_id = 0
            num_shards = 1
        
        self.input = File(
            file_root=data_dir,
            read_ahead=True,
            shuffle_after_epoch=True,
            shard_id=shard_id,
            num_shards=num_shards,
            initial_fill=20000,
            seed=seed,
            dont_use_mmap=dont_use_mmap
        )

        self.decode = Image(
            device="mixed",
            output_type=types.RGB,
            memory_stats=True,
        )

        self.res = RandomResizedCrop(
            device='gpu',
            size=(crop, crop),
            interp_type=interpolation,
            random_aspect_ratio=[0.75, 4.0/3.0],
            random_area=[0.08, 1.0],
            num_attempts=100,
            antialias=False,
            seed=seed,
        )
        self.cmnp = CropMirrorNormalize(
            device='gpu',
            dtype=types.DALIDataType.FLOAT,
            output_layout='CHW',
            crop=(crop, crop),
            mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],
            std=[0.229 * 255, 0.224 * 255, 0.225 * 255],
        )
        self.coin = CoinFlip(probability=0.5)
    
    def define_graph(self):
        self.jpegs, self.labels = self.input(name="Reader")
        images = self.decode(self.jpegs)
        images = self.res(images)
        images = self.cmnp(images, mirror=self.coin())
        return [images, self.labels]


class DaliImageNetValPipeline(Pipeline):
    def __init__(self,
                 batch_size: int,
                 data_dir: str,
                 interpolation: str,
                 resize: int,
                 crop: int,
                 dont_use_mmap: bool):
        super().__init__(batch_size, num_threads=2, device_id=0)
        interpolation = {
            "bicubic": types.INTERP_CUBIC,
            "bilinear": types.INTERP_LINEAR,
            "triangular": types.INTERP_TRIANGULAR,
        }[interpolation]

        if dist.is_initialized():
            shard_id = dist.get_rank()
            num_shards = dist.get_world_size()
        else:
            shard_id = 0
            num_shards = 1
        
        self.input = File(
            file_root=data_dir,
            random_shuffle=False,
            shard_id=shard_id,
            num_shards=num_shards,
            dont_use_mmap=dont_use_mmap,
            pad_last_batch=False,
        )

        self.decode = Image(
            device="mixed",
            output_type=types.RGB,
            memory_stats=True,
        )
        self.res = Resize(
            device='gpu',
            resize_shorter=resize,
            interp_type=interpolation,
            antialias=False
        )
        self.cmnp = CropMirrorNormalize(
            device='gpu',
            dtype=types.DALIDataType.FLOAT,
            output_layout='CHW',
            crop=(crop, crop),
            mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],
            std=[0.229 * 255, 0.224 * 255, 0.225 * 255],
        )

    def define_graph(self):
        self.jpegs, self.labels = self.input(name="Reader")
        images = self.decode(self.jpegs)
        images = self.res(images)
        images = self.cmnp(images.gpu())
        return [images, self.labels]


class DALIWrapper(object):
    def gen_wrapper(dalipipeline: DALIClassificationIterator):
        for data in dalipipeline:
            input = data[0]["data"].contiguous(memory_format=torch.contiguous_format)
            target = torch.reshape(data[0]["label"], [-1]).cuda().long()
            yield input, target
        dalipipeline.reset()

    def __init__(self, dalipipeline):
        self.dalipipeline = dalipipeline

    def __iter__(self):
        return DALIWrapper.gen_wrapper(self.dalipipeline)


def get_dali_train_loader(cfg: 'Config'):
    if cfg.data.alpha > 0:
        train_data_dir = os.path.join(cfg.data.data_dir, f"train_{dist.get_rank()}")
    else:
        train_data_dir = os.path.join(cfg.data.data_dir, "train")
    pipe = DaliImageNetTrainPipeline(
        batch_size=cfg.train.batch_size_per_local_batch,
        data_dir=train_data_dir,
        interpolation=cfg.train.preprocess.interpolation,
        crop=cfg.train.preprocess.train_crop_size,
        seed=cfg.train.reproduce.seed,
        heterogeneous=cfg.data.alpha > 0,
        dont_use_mmap=not cfg.train.preprocess.preload_local
    )
    pipe.build()
    train_loader = DALIClassificationIterator(
        pipe, reader_name="Reader", last_batch_policy=LastBatchPolicy.DROP
    )
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    logger.info(f"Epoch size: {pipe.epoch_size('Reader')}")
    if cfg.data.alpha > 0:
        return DALIWrapper(train_loader), int(pipe.epoch_size('Reader') / cfg.train.batch_size_per_local_batch)
    else:
        return DALIWrapper(train_loader), int(pipe.epoch_size('Reader') / cfg.train.batch_size_per_local_batch / world_size)


def get_dali_valid_loader(cfg: 'Config'):
    train_data_dir = os.path.join(cfg.data.data_dir, "val")
    pipe = DaliImageNetValPipeline(
        batch_size=cfg.train.batch_size_per_local_batch,
        data_dir=train_data_dir,
        interpolation=cfg.train.preprocess.interpolation,
        resize=cfg.train.preprocess.val_image_size,
        crop=cfg.train.preprocess.val_crop_size,
        dont_use_mmap=not cfg.train.preprocess.preload_local
    )
    pipe.build()
    valid_loader = DALIClassificationIterator(
        pipe,
        reader_name="Reader",
        last_batch_policy=LastBatchPolicy.PARTIAL
    )
    return DALIWrapper(valid_loader)

